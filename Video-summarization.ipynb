{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNYL1BRchMgXKBuRjh6alIl"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# **Lecture Script: Video Summarization using AI**\n","## **Introduction**\n","Hello everyone! Today, we are going to talk about an exciting application of Artificial Intelligence‚Äî**Video Summarization**.\n","\n","Have you ever watched a long video and wished you could get the key highlights in just a few minutes? That‚Äôs exactly what video summarization does!\n","\n","## **Why is Video Summarization Important?**\n","Imagine you have a 2-hour-long football match, but you only want to see the best moments‚Äîgoals, saves, and celebrations. Instead of manually watching and selecting clips, AI can do this for you automatically!\n","\n","## **Where is Video Summarization Used?**\n","This technology is widely used in:\n","\n","‚úÖ News Highlights ‚Äì AI can summarize long news reports into short clips.\n","\n","‚úÖ Sports Replays ‚Äì Only the best moments (like goals and saves) are extracted.\n","\n","‚úÖ Lecture Recordings ‚Äì AI can condense a 1-hour class into a 5-minute summary.\n","\n","‚úÖ Movie Trailers ‚Äì Automatically generate trailers by picking the most exciting scenes.\n","\n"],"metadata":{"id":"w_KxOleH3ilE"}},{"cell_type":"markdown","source":["## **How Does Video Summarization Work?**\n","The process of summarizing a video involves the following steps:\n","\n","1Ô∏è‚É£ Extract Key Frames ‚Äì Identify the most important moments from the video.\n","\n","2Ô∏è‚É£ Generate Captions ‚Äì Use AI to describe what is happening in each frame.\n","\n","3Ô∏è‚É£ Summarize the Story ‚Äì Use a language model (like GPT) to create a meaningful story.\n","\n","4Ô∏è‚É£ Create a Short Video ‚Äì Combine the key frames into a new video.\n","\n","5Ô∏è‚É£ Add Voice Narration ‚Äì Generate an audio summary using text-to-speech.\n","\n","Now, let‚Äôs go step by step through this process with hands-on coding! üöÄ"],"metadata":{"id":"Vl7Ieilw5Ft4"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q8X-GEU6hVQa","executionInfo":{"status":"ok","timestamp":1741280245713,"user_tz":-330,"elapsed":8262,"user":{"displayName":"Hina","userId":"01999978118778491773"}},"outputId":"746cfad7-a953-4a67-9a81-4971a92ed786"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["cd '/content/drive/MyDrive/Colab Notebooks/Video-Summarization/'\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ei3gnZtZNvgB","executionInfo":{"status":"ok","timestamp":1741280245713,"user_tz":-330,"elapsed":6,"user":{"displayName":"Hina","userId":"01999978118778491773"}},"outputId":"3e1d4143-df4a-47f1-ee43-3b95e07423f7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Colab Notebooks/Video-Summarization\n"]}]},{"cell_type":"code","source":["pwd"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"4RhQlC7fOTte","executionInfo":{"status":"ok","timestamp":1741280245713,"user_tz":-330,"elapsed":5,"user":{"displayName":"Hina","userId":"01999978118778491773"}},"outputId":"bd73cde3-1cb6-4f57-a99c-8101f5244b0e"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/content/drive/MyDrive/Colab Notebooks/Video-Summarization'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["!ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oW1Dv1h9Nank","executionInfo":{"status":"ok","timestamp":1741280252700,"user_tz":-330,"elapsed":5,"user":{"displayName":"Hina","userId":"01999978118778491773"}},"outputId":"65039b01-d71a-4bc8-d78a-57033ca1b62c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["app.py\t\t    Dockerfile\t   requirements.txt  summary_video.mp4\t  Video-summarization.ipynb\n","big-buck-bunny.mp4  narration.mp3  scenes\t     video_subtitles.srt\n"]}]},{"cell_type":"markdown","source":["# **Step 1: Extract Key Frames from a Video**\n","**What are Key Frames?**\n","\n","A key frame is a snapshot of an important moment in a video.\n","Instead of processing every single frame, we only keep the essential ones.\n","### **Method 1: Extract Frames at Regular Intervals**\n","We use OpenCV, a popular library for image and video processing.\n","\n","Example Code: Extract Frames Every 30 Frames"],"metadata":{"id":"7VKB7nLa4iS7"}},{"cell_type":"code","source":["import cv2\n","import os\n","\n","def extract_keyframes(video_path, output_folder, frame_interval=30):\n","    os.makedirs(output_folder, exist_ok=True)\n","    cap = cv2.VideoCapture(video_path)\n","    frame_count = 0\n","\n","    while cap.isOpened():\n","        ret, frame = cap.read()\n","        if not ret:\n","            break\n","\n","        if frame_count % frame_interval == 0:\n","            frame_path = os.path.join(output_folder, f\"frame_{frame_count}.jpg\")\n","            cv2.imwrite(frame_path, frame)\n","\n","        frame_count += 1\n","\n","    cap.release()\n","\n","# Example usage\n","extract_keyframes(\"big-buck-bunny.mp4\", \"frames\")\n"],"metadata":{"id":"59qKokYTXnUK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["üëâ This function extracts frames every 30 frames and saves them as images.\n","\n","### **Method 2: Extract Frames Using Scene Detection**\n","Instead of selecting frames randomly, we use PySceneDetect to detect real scene changes in the video.\n","\n","Example:\n","\n","\n","Imagine you are watching your favorite cartoon, and every time a new scene starts, you want to take a picture of it. This program does exactly that! It watches a video and automatically takes a snapshot whenever the scene changes. Just like when you see a new background or a different character in a cartoon, the program detects that and saves a picture. This helps in summarizing videos or picking out the important moments!"],"metadata":{"id":"_KH0WIG457Op"}},{"cell_type":"code","source":["\n","!pip install scenedetect[opencv] opencv-python\n","import os\n","import cv2\n","from scenedetect import open_video, SceneManager, ContentDetector\n","def save_scene_frames(video_path, output_folder):\n","    os.makedirs(output_folder, exist_ok=True)\n","    video = open_video(video_path)\n","\n","    scene_manager = SceneManager()\n","    scene_manager.add_detector(ContentDetector(threshold=27.0))  # Adjust sensitivity\n","    # Detect scenes\n","    scene_manager.detect_scenes(video)\n","    scenes = scene_manager.get_scene_list()\n","\n","    cap = cv2.VideoCapture(video_path)  # Open video for frame extraction\n","\n","    for i, (start, end) in enumerate(scenes):\n","        frame_time = start.get_frames()  # Extract frame at scene start\n","        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_time)\n","        ret, frame = cap.read()\n","\n","        if ret:\n","            frame_path = os.path.join(output_folder, f\"scene_{i+1}.jpg\")\n","            cv2.imwrite(frame_path, frame)\n","            print(f\"Saved: {frame_path}\")\n","\n","    cap.release()\n","\n","\n","\n","# Example usage\n","save_scene_frames(\"big-buck-bunny.mp4\", \"scenes\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kCq-bLhLquvE","executionInfo":{"status":"ok","timestamp":1741280267090,"user_tz":-330,"elapsed":14392,"user":{"displayName":"Hina","userId":"01999978118778491773"}},"outputId":"8ee2e11c-3d92-4422-c4e3-b9294940b5ec"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n","Requirement already satisfied: scenedetect[opencv] in /usr/local/lib/python3.11/dist-packages (0.6.5.2)\n","Requirement already satisfied: Click in /usr/local/lib/python3.11/dist-packages (from scenedetect[opencv]) (8.1.8)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from scenedetect[opencv]) (1.26.4)\n","Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from scenedetect[opencv]) (4.3.6)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from scenedetect[opencv]) (4.67.1)\n"]},{"output_type":"stream","name":"stderr","text":["INFO:pyscenedetect:Detecting scenes...\n"]},{"output_type":"stream","name":"stdout","text":["Saved: scenes/scene_1.jpg\n","Saved: scenes/scene_2.jpg\n","Saved: scenes/scene_3.jpg\n","Saved: scenes/scene_4.jpg\n","Saved: scenes/scene_5.jpg\n","Saved: scenes/scene_6.jpg\n","Saved: scenes/scene_7.jpg\n","Saved: scenes/scene_8.jpg\n","Saved: scenes/scene_9.jpg\n","Saved: scenes/scene_10.jpg\n","Saved: scenes/scene_11.jpg\n","Saved: scenes/scene_12.jpg\n","Saved: scenes/scene_13.jpg\n"]}]},{"cell_type":"markdown","source":["**Lecture Script: Scene Detection and Frame Extraction in Python**\n","\n","**Introduction**\n","\n","Hello everyone! Today, we are going to break down a Python script that detects scene changes in a video and extracts a representative frame for each scene. This script makes use of the `scenedetect` library along with `OpenCV`, which is a powerful tool for image and video processing.\n","\n","By the end of this lecture, you will understand:\n","\n","- How to install and import necessary libraries.\n","- How to use `scenedetect` to detect scene changes in a video.\n","- How to extract and save frames using `OpenCV`.\n","- How `ContentDetector` works internally.\n","\n","---\n","\n","### **Step 1: Installing Required Libraries**\n","\n","```python\n","!pip install scenedetect[opencv] opencv-python\n","```\n","\n","This command installs the required libraries:\n","\n","- `scenedetect[opencv]`: A Python library used for detecting scene changes in videos.\n","- `opencv-python`: OpenCV, which helps in reading and processing video frames.\n","\n","For Jupyter Notebook users, prefixing the command with `!` allows execution in the terminal.\n","\n","---\n","\n","### **Step 2: Importing Necessary Modules**\n","\n","```python\n","import os\n","import cv2\n","from scenedetect import open_video, SceneManager, ContentDetector\n","```\n","\n","- `os`: A built-in Python module to handle file and directory operations.\n","- `cv2`: The OpenCV library for video and image processing.\n","- `open_video`: A function from `scenedetect` to open video files.\n","- `SceneManager`: Manages the scene detection process.\n","- `ContentDetector`: A detector that identifies scene changes based on content differences.\n","\n","---\n","\n","### **Step 6: Setting Up Scene Detection**\n","\n","```python\n","scene_manager = SceneManager()\n","scene_manager.add_detector(ContentDetector(threshold=27.0))\n","```\n","\n","- `SceneManager()`: Initializes a scene detection manager.\n","- `add_detector(ContentDetector(threshold=27.0))`: Adds a content-based scene detector with a threshold of 27.0.\n","  - Lower values make it more sensitive (detecting more scenes).\n","  - Higher values make it less sensitive (detecting fewer scenes).\n","\n","### **How `ContentDetector` Works Internally**\n","\n","The `ContentDetector` works by analyzing the visual differences between consecutive frames of a video. It calculates a difference metric using histograms, which measure how much the color distribution changes from one frame to the next.\n","\n","#### **Example Explanation**\n","Imagine you have a flipbook where each page has a slightly different drawing. If you flip through it quickly, the differences between pages are small. But when you turn to a completely new scene, the drawing changes significantly. `ContentDetector` detects these large differences and marks them as scene changes.\n","\n","#### **Step-by-step Process**\n","1. It reads two consecutive frames from the video.\n","2. It converts each frame into a histogram (a mathematical representation of color distribution).\n","3. It calculates the difference between the histograms of the two frames.\n","4. If the difference exceeds the threshold (e.g., 27.0), it considers it a scene change.\n","5. The starting frame of each detected scene is recorded.\n","\n","#### **Real-World Analogy**\n","Think of watching a movie trailer. If two shots look very similar (e.g., a slow zoom on a face), they might be part of the same scene. But if it suddenly cuts to an explosion, that‚Äôs a big change‚Äîjust like `ContentDetector` identifying a new scene!\n","\n","---\n","\n","### **Summary**\n","This script:\n","\n","1. Installs and imports necessary libraries.\n","2. Opens the video file and sets up scene detection.\n","3. Detects scene changes using `ContentDetector` by comparing color histograms.\n","4. Extracts a frame at the beginning of each detected scene.\n","5. Saves the extracted frames as images.\n","\n","This method is useful for:\n","\n","- Automatic summarization of videos.\n","- Extracting key moments for analysis.\n","- Pre-processing data for machine learning tasks.\n","\n","I hope this breakdown helped you understand the script! Feel free to ask any questions. Happy coding!\n","\n","\n"],"metadata":{"id":"fDMLdKFW6RJp"}},{"cell_type":"markdown","source":["## **Step 2: Generate Captions for Frames**\n","Once we have extracted the key frames, we need to describe what is happening in each image using AI.\n","\n","### **What is BLIP?**\n","BLIP (Bootstrapped Language-Image Pretraining) is an AI model that generates text captions from images."],"metadata":{"id":"Idioednr7yDV"}},{"cell_type":"code","source":[],"metadata":{"id":"aOqk3RlUUNEa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import BlipProcessor, BlipForConditionalGeneration\n","from PIL import Image\n","import os\n","\n","# Load BLIP Model for Image Captioning\n","processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n","model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n","\n","def generate_image_caption(image_path):\n","    image = Image.open(image_path).convert(\"RGB\")\n","    inputs = processor(image, return_tensors=\"pt\")\n","    caption_ids = model.generate(**inputs)\n","    caption = processor.decode(caption_ids[0], skip_special_tokens=True)\n","    return caption\n","\n","# Example Usage: Generate captions for all extracted frames\n","image_folder = \"scenes\"\n","captions = []\n","\n","for filename in sorted(os.listdir(image_folder)):  # Sort to maintain order\n","    if filename.endswith(\".jpg\"):\n","        image_path = os.path.join(image_folder, filename)\n","        caption = generate_image_caption(image_path)\n","        captions.append(f\"{filename}: {caption}\")\n","\n","# Combine captions into a single text\n","  # This will be sent to ChatGPT\n","video_summary_input = \"\\n\".join(captions)\n","print(video_summary_input)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Jlso_TWhw-nY","executionInfo":{"status":"ok","timestamp":1741280364742,"user_tz":-330,"elapsed":97655,"user":{"displayName":"Hina","userId":"01999978118778491773"}},"outputId":"6c63e5c3-67e9-415f-bef8-c7baf98efb95"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n","\n"]},{"output_type":"stream","name":"stdout","text":["scene_1.jpg: a rabbit is sitting in the grass in a forest\n","scene_10.jpg: a white rabbit standing next to a tree\n","scene_11.jpg: a white rabbit standing in a field with a red ball\n","scene_12.jpg: a man is standing in the grass next to a tree\n","scene_13.jpg: a white rabbit standing in a field\n","scene_2.jpg: a tree with leaves\n","scene_3.jpg: the secret in the secret secret secret secret secret secret secret secret secret secret secret secret secret secret secret secret\n","scene_4.jpg: a white rabbit standing in a field of flowers\n","scene_5.jpg: a pig standing in a field of grass\n","scene_6.jpg: a white bird is perched on a tree branch\n","scene_7.jpg: a rabbit is standing in the middle of a field of flowers\n","scene_8.jpg: a white rabbit is running in the grass\n","scene_9.jpg: a white rabbit standing in a field with a butterfly flying above\n"]}]},{"cell_type":"markdown","source":["**Lecture Script: Understanding BLIP for Image Captioning**\n","\n","---\n","\n","## **Introduction**\n","Hello everyone, and welcome to today's lecture! In this session, we will be exploring how to use a powerful AI model called BLIP (Bootstrapped Language-Image Pretraining) to generate captions for images.\n","\n","By the end of this lecture, you will understand:\n","- What BLIP is and how it works.\n","- The purpose of each line of the given code.\n","- How to process multiple images to generate captions.\n","\n","Let's get started!\n","\n","---\n","\n","## **Step 1: Importing Required Libraries**\n","```python\n","from transformers import BlipProcessor, BlipForConditionalGeneration\n","from PIL import Image\n","import os\n","```\n","### **Explanation:**\n","1. **`transformers` Library:** This is a Hugging Face library that provides state-of-the-art models for Natural Language Processing (NLP) and Vision-Language tasks.\n","    - `BlipProcessor`: Prepares input data for the BLIP model by converting images and text into a format the model understands.\n","    - `BlipForConditionalGeneration`: The pre-trained BLIP model used for generating image captions.\n","2. **`PIL (Python Imaging Library)`**: Used for opening and processing images.\n","3. **`os` Module**: Helps in navigating directories and working with files.\n","\n","#### **Example:**\n","If you are working with images stored in a folder, the `os` module allows you to loop through all image files and process them automatically.\n","\n","---\n","\n","## **Step 2: Loading the BLIP Model**\n","```python\n","processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n","model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n","```\n","### **Explanation:**\n","1. **Loading the Pre-Trained Processor**:\n","    - `BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")` downloads and loads the BLIP processor, which helps in preparing input images.\n","2. **Loading the Pre-Trained Model**:\n","    - `BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")` loads the BLIP model specifically designed for captioning images.\n","\n","This means we are now ready to process images and generate meaningful captions!\n","\n","---\n","\n","## **Step 3: Creating a Function to Generate Captions**\n","```python\n","def generate_image_caption(image_path):\n","    image = Image.open(image_path).convert(\"RGB\")\n","    inputs = processor(image, return_tensors=\"pt\")\n","    caption_ids = model.generate(**inputs)\n","    caption = processor.decode(caption_ids[0], skip_special_tokens=True)\n","    return caption\n","```\n","### **Breaking it Down:**\n","1. **Open the Image**:\n","   - `Image.open(image_path).convert(\"RGB\")`: Opens the image file and ensures it is in RGB format (3 color channels: Red, Green, and Blue).\n","2. **Process the Image**:\n","   - `inputs = processor(image, return_tensors=\"pt\")`: Converts the image into a format the model can understand (PyTorch tensors).\n","3. **Generate Caption**:\n","   - `caption_ids = model.generate(**inputs)`: Uses the BLIP model to predict a caption for the image.\n","4. **Decode the Caption**:\n","   - `caption = processor.decode(caption_ids[0], skip_special_tokens=True)`: Converts the generated token IDs into a human-readable caption.\n","5. **Return the Caption**:\n","   - The function outputs the generated caption as a string.\n","\n","#### **Example Usage:**\n","If you pass an image named `example.jpg`, this function will return a caption describing the image.\n","\n","---\n","\n","## **Step 4: Processing Multiple Images**\n","```python\n","image_folder = \"scenes\"\n","captions = []\n","\n","for filename in sorted(os.listdir(image_folder)):  # Sort to maintain order\n","    if filename.endswith(\".jpg\"):\n","        image_path = os.path.join(image_folder, filename)\n","        caption = generate_image_caption(image_path)\n","        captions.append(f\"{filename}: {caption}\")\n","```\n","### **What Happens Here?**\n","1. **Define the Folder Containing Images**:\n","   - `image_folder = \"scenes\"`: Specifies the directory where images are stored.\n","2. **Create an Empty List to Store Captions**:\n","   - `captions = []`: Stores generated captions for each image.\n","3. **Loop Through Images in the Folder**:\n","   - `sorted(os.listdir(image_folder))`: Lists all files and sorts them alphabetically to maintain order.\n","   - `if filename.endswith(\".jpg\")`: Ensures only `.jpg` files are processed.\n","4. **Generate Captions for Each Image**:\n","   - `image_path = os.path.join(image_folder, filename)`: Creates the full path of the image file.\n","   - `generate_image_caption(image_path)`: Calls our function to generate a caption.\n","   - `captions.append(f\"{filename}: {caption}\")`: Stores the filename along with its generated caption.\n","\n","#### **Example Output:**\n","If the folder contains `image1.jpg`, `image2.jpg`, and `image3.jpg`, the captions list may look like:\n","```\n","['image1.jpg: A dog playing in the park.',\n"," 'image2.jpg: A sunset over the mountains.',\n"," 'image3.jpg: A person riding a bicycle.']\n","```\n","\n","---\n","\n","## **Step 5: Preparing the Final Output**\n","```python\n","video_summary_input = \"\\n\".join(captions)\n","print(video_summary_input)\n","```\n","### **Explanation:**\n","1. **Join Captions into a Single Text**:\n","   - `\"\\n\".join(captions)`: Combines all captions into a single string, with each caption on a new line.\n","2. **Print the Final Output**:\n","   - `print(video_summary_input)`: Displays the generated captions.\n","\n","#### **Example Output:**\n","```\n","image1.jpg: A dog playing in the park.\n","image2.jpg: A sunset over the mountains.\n","image3.jpg: A person riding a bicycle.\n","```\n","This combined text can be used as an input to a chatbot or a summarization model.\n","\n","---\n","\n","## **Summary**\n","- We used the BLIP model to generate image captions.\n","- We processed images from a folder and generated meaningful descriptions.\n","- We combined captions into a structured text summary.\n","\n","This is a powerful technique that can be applied in automated video summarization, accessibility features, and content tagging.\n","\n","That‚Äôs it for today! If you have any questions, feel free to ask. Happy coding! üöÄ\n","\n"],"metadata":{"id":"WgNeDSq88Tx6"}},{"cell_type":"markdown","source":["#Detailed Explanation"],"metadata":{"id":"XNydstv98r1R"}},{"cell_type":"markdown","source":["### **How BLIP Works Internally to Generate Image Captions**  \n","\n","The BLIP (Bootstrapped Language-Image Pretraining) model is designed to understand images and generate natural language descriptions based on what it sees. Let‚Äôs break down how it works internally step by step.\n","\n","---\n","\n","## **1. Overview of BLIP's Architecture**\n","BLIP is a vision-language model that combines an image encoder and a text decoder. Internally, it operates using **two main components**:\n","\n","1. **Vision Encoder (Image Understanding)**\n","   - This part of the model processes the input image to extract meaningful visual features.\n","   - It uses a **Vision Transformer (ViT)**, which converts an image into a sequence of feature representations (similar to how text is tokenized in NLP models).\n","   \n","2. **Text Decoder (Caption Generation)**\n","   - This component takes the extracted visual features and generates a human-readable caption.\n","   - It is based on a **Transformer-based language model**, similar to GPT or BERT, which generates text word-by-word.\n","\n","---\n","\n","## **2. Step-by-Step Process of Image Captioning**\n","\n","Let's go step by step through how BLIP processes an image to generate a caption.\n","\n","### **Step 1: Preprocessing the Image**\n","- The image is first converted to an RGB format and resized to a standard shape.\n","- It is then transformed into a tensor, which is the numerical representation of the image.\n","\n","**Example:**\n","If the input image is a picture of a cat sitting on a chair, the preprocessing step converts it into a numerical format that the model can understand.\n","\n","### **Step 2: Feature Extraction with the Vision Encoder**\n","- The pre-trained **Vision Transformer (ViT)** extracts key features from the image.\n","- These features capture objects, textures, colors, and spatial relationships.\n","\n","**Example:**\n","The model identifies that the image contains a \"cat,\" \"chair,\" and \"background details.\"\n","\n","### **Step 3: Generating the Caption with the Text Decoder**\n","- The text decoder, a transformer-based model, takes the extracted image features and generates a caption.\n","- It uses an **auto-regressive generation process**, meaning it predicts one word at a time based on previous words and visual features.\n","\n","**Example:**\n","1. The model starts with a special start token (`[CLS]`).\n","2. It predicts the first word, e.g., `\"A\"`.\n","3. Based on `\"A\"` and the image features, it predicts the next word, e.g., `\"cat\"`.\n","4. It continues predicting words until it reaches a stop condition.\n","\n","**Final Caption:** `\"A cat sitting on a chair.\"`\n","\n","---\n","\n","## **3. Detailed Example with Input and Output**\n","### **Input**\n","Imagine we give BLIP the following image:\n","\n","üì∑ *(An image of a dog playing with a ball in the park.)*\n","\n","### **Processing Internally**\n","- The vision encoder processes the image and extracts key features:\n","  ```\n","  [Dog, Ball, Grass, Sky]\n","  ```\n","- The text decoder generates a caption step by step:\n","  ```\n","  \"A\" ‚Üí \"dog\" ‚Üí \"playing\" ‚Üí \"with\" ‚Üí \"a\" ‚Üí \"ball\" ‚Üí \"in\" ‚Üí \"the\" ‚Üí \"park.\"\n","  ```\n","\n","### **Output**\n","The final generated caption:\n","```\n","\"A dog playing with a ball in the park.\"\n","```\n","\n","---\n","\n","## **4. Why is BLIP Effective?**\n","- **Pretraining on Large Datasets**: BLIP is trained on millions of image-text pairs, enabling it to learn rich relationships between vision and language.\n","- **Transformer-Based Architecture**: Uses powerful transformer models to understand context and generate coherent captions.\n","- **Fine-Tuning Capabilities**: Can be adapted for specific tasks like detailed descriptions, storytelling, or question answering.\n","\n","---\n","\n","## **5. Summary**\n","- **BLIP first encodes the image using a Vision Transformer (ViT).**\n","- **It then passes extracted features to a Transformer-based text decoder.**\n","- **The model generates a caption word by word based on the image features.**\n","- **The final caption describes the most important elements of the image in natural language.**\n","\n","This is how BLIP internally works to generate image captions! üöÄ Let me know if you want further clarifications."],"metadata":{"id":"8RdGGlqQ8ZxP"}},{"cell_type":"markdown","source":["üëâ Now we have text descriptions of each extracted frame!\n","\n","## **Step 3: Summarize the Captions into a Story**\n","Once we have captions for all frames, we need to turn them into a coherent story.\n","\n","Example Code: Using GPT to Summarize"],"metadata":{"id":"yhv8QTC09VIk"}},{"cell_type":"code","source":["from openai import OpenAI\n","\n","client = OpenAI(\n","  api_key=\"********\"\n",")\n","\n","\n","def summarize_video(captions):\n","    prompt = f\"Summarize the following sequence of video frames into a meaningful story:\\n\\n{captions}\"\n","\n","    completion = client.chat.completions.create(\n","        model=\"gpt-4o-mini\",\n","        messages=[{\"role\": \"system\", \"content\": \"You are an AI that summarizes video content.\"},\n","                  {\"role\": \"user\", \"content\": prompt}]\n","    )\n","\n","    return completion.choices[0].message.content\n","\n","# Get the summary\n","video_summary = summarize_video(video_summary_input)\n","print(video_summary)\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pAtgYdYD2IrB","executionInfo":{"status":"ok","timestamp":1741280369403,"user_tz":-330,"elapsed":4669,"user":{"displayName":"Hina","userId":"01999978118778491773"}},"outputId":"222ec1c9-5e2a-4f5b-8bb1-e11b45cba681"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["In a serene forest, a curious white rabbit finds its way through a vibrant landscape. It begins by sitting among the grass (scene_1.jpg) before exploring further. Eventually, it encounters a towering tree (scene_10.jpg) and a colorful field with a playful red ball (scene_11.jpg), delighting in the simple joys of nature.\n","\n","As the day progresses, the rabbit wanders into a lush field (scene_13.jpg) filled with flowers and the gentle company of a pig grazing nearby (scene_5.jpg). Inspiration strikes as it notices a white bird perched peacefully in the branches above (scene_6.jpg), emphasizing the beauty of their coexistence.\n","\n","The adventure continues with the rabbit dancing through the flowers (scene_4.jpg) and discovering its playful side, as it gleefully runs through the grass (scene_8.jpg) while a butterfly flutters overhead (scene_9.jpg). All the while, a man stands nearby, admiring the scene and perhaps pondering the secrets of nature (scene_12.jpg, scene_3.jpg).\n","\n","This whimsical tale captures the essence of exploration, friendship, and the magic that unfolds in a hidden corner of the forest... all through the eyes of a white rabbit.\n"]}]},{"cell_type":"markdown","source":["üëâ Now, GPT will generate a short summary of the video!"],"metadata":{"id":"D8JAKRQe9mQY"}},{"cell_type":"markdown","source":["**Lecture Script: Understanding the OpenAI Video Summarization Code**\n","\n","### Introduction\n","\n","Hello everyone! Today, we will break down a Python script that interacts with OpenAI‚Äôs API to summarize video captions. This script will take text-based descriptions of video frames and generate a meaningful summary.\n","\n","By the end of this lecture, you will understand:\n","\n","1. How to import and use OpenAI‚Äôs API in Python.\n","2. How to create an API client for making requests.\n","3. How to structure prompts for AI-based summarization.\n","4. How to extract and display the response from OpenAI.\n","5. How ChatGPT-4 processes input and generates meaningful summaries.\n","\n","---\n","\n","### **Step 1: Importing the Required Library**\n","\n","```python\n","from openai import OpenAI\n","```\n","\n","#### **Explanation:**\n","\n","- This line imports the `OpenAI` class from the `openai` module.\n","- The `OpenAI` library provides an interface to communicate with OpenAI‚Äôs models, such as GPT-4.\n","- Make sure you have the OpenAI package installed using:\n","  ```bash\n","  pip install openai\n","  ```\n","\n","---\n","\n","### **Step 2: Setting Up the OpenAI Client**\n","\n","```python\n","client = OpenAI(\n","  api_key=\"your-api-key-here\"\n",")\n","```\n","\n","#### **Explanation:**\n","\n","- We create an `OpenAI` client object, which will be used to send requests to OpenAI‚Äôs API.\n","- The `api_key` is required to authenticate the request. Replace \"your-api-key-here\" with your actual API key.\n","- Never share your API key publicly as it provides access to OpenAI‚Äôs services and can incur costs.\n","\n","**Example:** If a user wants to connect with OpenAI, they would initialize the client as:\n","\n","```python\n","client = OpenAI(api_key=\"sk-XXXXXX\")\n","```\n","\n","---\n","\n","### **Step 3: Creating the Video Summarization Function**\n","\n","```python\n","def summarize_video(captions):\n","    prompt = f\"Summarize the following sequence of video frames into a meaningful story:\\n\\n{captions}\"\n","```\n","\n","#### **Explanation:**\n","\n","- We define a function `summarize_video(captions)` which takes `captions` (text extracted from a video) as input.\n","- A `prompt` is created using an f-string. It asks the AI model to summarize the video captions.\n","- The `\\n\\n` ensures proper formatting for readability.\n","\n","**Example:** If `captions` contains:\n","\n","```python\n","captions = \"A man walks into a store. He picks up an apple and pays at the counter.\"\n","```\n","\n","The `prompt` will be:\n","\n","```python\n","\"Summarize the following sequence of video frames into a meaningful story:\\n\\nA man walks into a store. He picks up an apple and pays at the counter.\"\n","```\n","\n","---\n","\n","### **Step 4: Sending a Request to OpenAI‚Äôs Chat Model**\n","\n","```python\n","    completion = client.chat.completions.create(\n","        model=\"gpt-4o-mini\",\n","        messages=[\n","            {\"role\": \"system\", \"content\": \"You are an AI that summarizes video content.\"},\n","            {\"role\": \"user\", \"content\": prompt}\n","        ]\n","    )\n","```\n","\n","#### **Explanation:**\n","\n","- We call `client.chat.completions.create()` to send a request to OpenAI.\n","- `model=\"gpt-4o-mini\"` specifies the model used.\n","- `messages` is a list containing:\n","  - A system message: Defines AI‚Äôs role as a video summarizer.\n","  - A user message: Contains the actual prompt with captions.\n","- The AI processes this request and generates a response.\n","\n","---\n","\n","### **Step 5: Extracting and Returning the AI‚Äôs Response**\n","\n","```python\n","    return completion.choices[0].message.content\n","```\n","\n","#### **Explanation:**\n","\n","- `completion.choices[0]` accesses the first (and usually only) response.\n","- `.message.content` extracts the generated summary text.\n","- The function returns the summarized content.\n","\n","---\n","\n","### **Step 6: Calling the Function and Printing the Summary**\n","\n","```python\n","video_summary = summarize_video(video_summary_input)\n","print(video_summary)\n","```\n","\n","#### **Explanation:**\n","\n","- `summarize_video(video_summary_input)` calls our function with an input text (`video_summary_input`).\n","- The returned summary is stored in `video_summary`.\n","- `print(video_summary)` displays the AI-generated summary.\n","\n","**Example Output:** If `video_summary_input` is:\n","\n","```python\n","\"A man enters a store, looks around, and buys an apple.\"\n","```\n","\n","The printed output could be:\n","\n","```python\n","\"A man visits a store and purchases an apple.\"\n","```\n","\n","---\n","\n","### **Step 7: How ChatGPT-4 Generates a Meaningful Summary**\n","\n","To understand how ChatGPT-4 processes captions and generates a story, let's break it down:\n","\n","1. **Tokenization:**\n","\n","   - The input text (captions) is broken down into smaller units called tokens (words, subwords, or characters).\n","   - Example: \"A man walks into a store.\" ‚Üí [\"A\", \"man\", \"walks\", \"into\", \"a\", \"store\"]\n","\n","2. **Context Understanding:**\n","\n","   - GPT-4 uses deep learning to analyze the sequence of tokens and extract the meaning.\n","   - It identifies key entities (e.g., \"man\", \"store\") and their actions (\"walks into\", \"buys\").\n","\n","3. **Pattern Recognition:**\n","\n","   - The model has been trained on large datasets containing stories, summaries, and structured narratives.\n","   - It compares the input with similar patterns it has learned before.\n","\n","4. **Coherent Generation:**\n","\n","   - The AI predicts the next most likely words based on context.\n","   - It restructures the input into a smooth and concise story.\n","\n","5. **Post-Processing:**\n","\n","   - The final output is checked for fluency and logical coherence before being returned.\n","\n","**Example Breakdown:** Input Captions:\n","\n","```python\n","\"A boy kicks a ball. The ball hits a window. The window breaks.\"\n","```\n","\n","Processing Steps:\n","\n","- Identifies key elements: (Boy ‚Üí kicks ‚Üí Ball ‚Üí hits ‚Üí Window ‚Üí breaks)\n","- Recognizes cause-effect relationships.\n","- Generates a structured summary:\n","\n","```python\n","\"A boy accidentally breaks a window while playing with a ball.\"\n","```\n","\n","This is how GPT-4 transforms scattered captions into a meaningful story!\n","\n","---\n","\n","### **Conclusion**\n","\n","In today‚Äôs lesson, we learned:\n","\n","- How to import and set up OpenAI‚Äôs API.\n","- How to structure a function to summarize video captions.\n","- How to format and send a request to OpenAI.\n","- How to extract and print the AI-generated response.\n","- How ChatGPT-4 processes text internally to generate meaningful summaries.\n","\n","This script is a great starting point for AI-based video summarization. You can enhance it by integrating it with video-to-text tools like Whisper for automatic transcript generation.\n","\n","Happy coding!\n","\n"],"metadata":{"id":"aCv6WjBZ-J54"}},{"cell_type":"markdown","source":["## **Step 4: Convert Frames into a Short Video**\n","Now, we combine the key frames into a new summarized video.\n","\n","Example Code: Creating Summary Video"],"metadata":{"id":"ek3RBwyt9nR-"}},{"cell_type":"code","source":["import moviepy.editor as mp\n","\n","def create_summary_video(image_folder, output_video):\n","    images = sorted([os.path.join(image_folder, img) for img in os.listdir(image_folder) if img.endswith(\".jpg\")])\n","    clips = [mp.ImageClip(img).set_duration(2) for img in images]  # 2 sec per frame\n","\n","    video = mp.concatenate_videoclips(clips, method=\"compose\")\n","    video.write_videofile(output_video, fps=24)\n","\n","# Example usage\n","create_summary_video(\"scenes\", \"summary_video.mp4\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lCI7PxwoUk6O","executionInfo":{"status":"ok","timestamp":1741280396006,"user_tz":-330,"elapsed":26606,"user":{"displayName":"Hina","userId":"01999978118778491773"}},"outputId":"90935476-d6a4-4cd4-dd8c-34b0fa8fb220"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Moviepy - Building video summary_video.mp4.\n","Moviepy - Writing video summary_video.mp4\n","\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Moviepy - Done !\n","Moviepy - video ready summary_video.mp4\n"]}]},{"cell_type":"markdown","source":["## **Step 5: Add Voice Narration**\n","We use gTTS (Google Text-to-Speech) to add a voice-over to the video."],"metadata":{"id":"FMV8Kqk39vKq"}},{"cell_type":"code","source":["!pip install gtts"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"swwcV1r1PzYx","executionInfo":{"status":"ok","timestamp":1741280400509,"user_tz":-330,"elapsed":4525,"user":{"displayName":"Hina","userId":"01999978118778491773"}},"outputId":"32b52760-c4d0-4bc1-9221-c88a60680212"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: gtts in /usr/local/lib/python3.11/dist-packages (2.5.4)\n","Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from gtts) (2.32.3)\n","Requirement already satisfied: click<8.2,>=7.1 in /usr/local/lib/python3.11/dist-packages (from gtts) (8.1.8)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->gtts) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->gtts) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->gtts) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->gtts) (2025.1.31)\n"]}]},{"cell_type":"code","source":["from gtts import gTTS\n","\n","def generate_voice_narration(captions, output_audio):\n","    text = \"\".join(captions)\n","    print(text)\n","    tts = gTTS(text, lang=\"en\")\n","    tts.save(output_audio)\n","\n","# Example usage\n","generate_voice_narration(video_summary, \"narration.mp3\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a9uLNtRZTcCQ","executionInfo":{"status":"ok","timestamp":1741280406835,"user_tz":-330,"elapsed":6334,"user":{"displayName":"Hina","userId":"01999978118778491773"}},"outputId":"8ff49999-10fd-4860-caf3-88267468a245"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["In a serene forest, a curious white rabbit finds its way through a vibrant landscape. It begins by sitting among the grass (scene_1.jpg) before exploring further. Eventually, it encounters a towering tree (scene_10.jpg) and a colorful field with a playful red ball (scene_11.jpg), delighting in the simple joys of nature.\n","\n","As the day progresses, the rabbit wanders into a lush field (scene_13.jpg) filled with flowers and the gentle company of a pig grazing nearby (scene_5.jpg). Inspiration strikes as it notices a white bird perched peacefully in the branches above (scene_6.jpg), emphasizing the beauty of their coexistence.\n","\n","The adventure continues with the rabbit dancing through the flowers (scene_4.jpg) and discovering its playful side, as it gleefully runs through the grass (scene_8.jpg) while a butterfly flutters overhead (scene_9.jpg). All the while, a man stands nearby, admiring the scene and perhaps pondering the secrets of nature (scene_12.jpg, scene_3.jpg).\n","\n","This whimsical tale captures the essence of exploration, friendship, and the magic that unfolds in a hidden corner of the forest... all through the eyes of a white rabbit.\n"]}]},{"cell_type":"markdown","source":["# **Conclusion**\n","‚úÖ Extracted key frames from a video\n","\n","‚úÖ Generated captions using AI\n","\n","‚úÖ Summarized the captions into a story\n","\n","‚úÖ Created a short video with highlights\n","\n","‚úÖ Added voice narration\n","\n","This is how AI can automatically summarize long videos into short, meaningful clips! üöÄ"],"metadata":{"id":"k4dKX6Zc95H5"}},{"cell_type":"code","source":["!pip install streamlit opencv-python transformers torch pillow moviepy gtts scenedetect[opencv]\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"04-GA52hIQ9u","executionInfo":{"status":"ok","timestamp":1741108198255,"user_tz":-330,"elapsed":103075,"user":{"displayName":"Hina","userId":"01999978118778491773"}},"outputId":"b8d2b920-32f0-4981-993f-662468252c67"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting streamlit\n","  Downloading streamlit-1.42.2-py2.py3-none-any.whl.metadata (8.9 kB)\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.3)\n","Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (11.1.0)\n","Requirement already satisfied: moviepy in /usr/local/lib/python3.11/dist-packages (1.0.3)\n","Requirement already satisfied: gtts in /usr/local/lib/python3.11/dist-packages (2.5.4)\n","Requirement already satisfied: scenedetect[opencv] in /usr/local/lib/python3.11/dist-packages (0.6.5.2)\n","Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.0)\n","Requirement already satisfied: blinker<2,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.9.0)\n","Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.2)\n","Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.1.8)\n","Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.26.4)\n","Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (24.2)\n","Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.2.2)\n","Requirement already satisfied: protobuf<6,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.25.6)\n","Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (18.1.0)\n","Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.32.3)\n","Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (13.9.4)\n","Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (9.0.0)\n","Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.10.2)\n","Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.12.2)\n","Collecting watchdog<7,>=2.1.5 (from streamlit)\n","  Downloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl.metadata (44 kB)\n","\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.1.44)\n","Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n","  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n","Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.4.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.28.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n","Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n","  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n","  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n","  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n","  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n","  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n","  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n","  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n","  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n","  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n","  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n","Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.11/dist-packages (from moviepy) (4.4.2)\n","Requirement already satisfied: imageio<3.0,>=2.5 in /usr/local/lib/python3.11/dist-packages (from moviepy) (2.37.0)\n","Requirement already satisfied: imageio_ffmpeg>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from moviepy) (0.6.0)\n","Requirement already satisfied: proglog<=1.0.0 in /usr/local/lib/python3.11/dist-packages (from moviepy) (0.1.10)\n","Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from scenedetect[opencv]) (4.3.6)\n","Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n","Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (1.28.0)\n","Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.1)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2025.1.31)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich<14,>=10.14.0->streamlit) (2.18.0)\n","Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n","Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.1.0)\n","Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2024.10.1)\n","Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\n","Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.23.1)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n","Downloading streamlit-1.42.2-py2.py3-none-any.whl (9.6 MB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m102.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m104.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m80.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m51.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m84.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m105.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl (79 kB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: watchdog, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, pydeck, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, streamlit\n","  Attempting uninstall: nvidia-nvjitlink-cu12\n","    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n","    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n","      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n","  Attempting uninstall: nvidia-curand-cu12\n","    Found existing installation: nvidia-curand-cu12 10.3.6.82\n","    Uninstalling nvidia-curand-cu12-10.3.6.82:\n","      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n","  Attempting uninstall: nvidia-cufft-cu12\n","    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n","    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n","      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n","  Attempting uninstall: nvidia-cuda-runtime-cu12\n","    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n","    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n","    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n","    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-cupti-cu12\n","    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n","    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n","  Attempting uninstall: nvidia-cublas-cu12\n","    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n","    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n","      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n","  Attempting uninstall: nvidia-cusparse-cu12\n","    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n","    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n","      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n","  Attempting uninstall: nvidia-cudnn-cu12\n","    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n","    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n","      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n","  Attempting uninstall: nvidia-cusolver-cu12\n","    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n","    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n","      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n","Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pydeck-0.9.1 streamlit-1.42.2 watchdog-6.0.0\n"]}]},{"cell_type":"code","source":["file_name = 'requirements.txt'\n","file_path = folder_path + file_name\n","\n","# Open the file and write to it\n","with open(file_path, 'w') as f:\n","    f.write('This is a sample text file created using Google Colab.')\n","\n","print(f\"File saved to: {file_path}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8OP4vPGHhnzo","executionInfo":{"status":"ok","timestamp":1741081254430,"user_tz":-330,"elapsed":3,"user":{"displayName":"Hina","userId":"01999978118778491773"}},"outputId":"5550ad7a-684a-46e5-be5f-2009261cb017"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["File saved to: /content/drive/MyDrive/Colab Notebooks/Video-Summarization/requirements.txt\n"]}]},{"cell_type":"code","source":["file_name = 'app.py'\n","file_path = folder_path + file_name\n","\n","# Open the file and write to it\n","with open(file_path, 'w') as f:\n","    f.write('Hello This is app.py')\n","\n","print(f\"File saved to: {file_path}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hFAhyNIMMrRT","executionInfo":{"status":"ok","timestamp":1741109260585,"user_tz":-330,"elapsed":606,"user":{"displayName":"Hina","userId":"01999978118778491773"}},"outputId":"762bc603-5484-4796-b3f8-a12a070b463a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["File saved to: /content/drive/MyDrive/Colab Notebooks/Video-Summarization/app.py\n"]}]},{"cell_type":"code","source":["dockerfile_content = \"\"\"\n","# Use Python base image\n","FROM python:3.9\n","\n","# Set working directory\n","WORKDIR /app\n","\n","# Copy all project files\n","COPY . .\n","\n","# Install dependencies\n","RUN pip install --no-cache-dir -r requirements.txt\n","\n","# Expose Streamlit port\n","EXPOSE 8501\n","\n","# Run the Streamlit app\n","CMD [\"streamlit\", \"run\", \"app.py\", \"--server.port=8501\", \"--server.address=0.0.0.0\"]\n","\"\"\"\n","\n","# Define the file path for the Dockerfile\n","dockerfile_path = folder_path + 'Dockerfile'\n","\n","# Open the file and write the Dockerfile content\n","with open(dockerfile_path, 'w') as f:\n","    f.write(dockerfile_content)\n","\n","print(f\"Dockerfile saved to: {dockerfile_path}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cq4_IeRBh546","executionInfo":{"status":"ok","timestamp":1741081463655,"user_tz":-330,"elapsed":442,"user":{"displayName":"Hina","userId":"01999978118778491773"}},"outputId":"3fe15b66-65b6-4cab-e008-5d5aec9921bf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Dockerfile saved to: /content/drive/MyDrive/Colab Notebooks/Video-Summarization/Dockerfile\n"]}]},{"cell_type":"code","source":["!ls\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WUNM817wit7p","executionInfo":{"status":"ok","timestamp":1741108489870,"user_tz":-330,"elapsed":953,"user":{"displayName":"Hina","userId":"01999978118778491773"}},"outputId":"6b9c49ae-bb07-4725-8015-a65068b39324"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["app.py\tbig-buck-bunny.mp4  drive  frames  narration.mp3  sample_data  scenes\n"]}]}]}
